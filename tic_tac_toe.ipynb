{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from board import TicTacToe_Board\n",
    "from DQN import DQN\n",
    "from Agent import RandomAgent, AIAgent\n",
    "from hard_code import hardcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "# Define replay memory:\n",
    "Transition = namedtuple('Transition',\\\n",
    "        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch):\n",
    "        return random.sample(self.memory, batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(policy, target, memory, BATCH, optimizer):\n",
    "    if memory.__len__() < BATCH:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Separate states, rewards,...\n",
    "    states = torch.cat(batch.state).reshape(BATCH, -1)\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "    next_state = torch.cat(batch.next_state).reshape(BATCH, -1)\n",
    "    done = batch.done\n",
    "    \n",
    "    # Evaluate policy on states:\n",
    "    out_policy = policy(states).gather(1, actions)\n",
    "    \n",
    "    # Evaluate target:\n",
    "    out_target = target(next_state).max(1)[0]\n",
    "    \n",
    "    for elem in range(out_target.size(0)):\n",
    "        if done[elem]: out_target *= 0.\n",
    "    \n",
    "    target = (rewards + GAMMA * out_target).view(BATCH,1)\n",
    "    loss = torch.mean(torch.pow(out_policy - target,2))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "     \n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = 10000\n",
    "BATCH = 100\n",
    "EPISODES = 200000\n",
    "GAMMA = 0.99\n",
    "UPDATE = 10\n",
    "IT_PRINT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greddy parameters:\n",
    "EPSmax = 6e-1\n",
    "EPSmin = 1e-1\n",
    "decay = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyPolicy(epoch, EPSmax, EPSmin, decay):\n",
    "    return  (EPSmax - EPSmin) * np.exp(-decay * epoch) + EPSmin\n",
    "\n",
    "def select_move(policy, state, EPS=0.):\n",
    "    if random.random() < EPS:\n",
    "        return torch.randint(0,5,(1,1))\n",
    "    else:\n",
    "        policy_value = policy(torch.Tensor(state))\n",
    "        return policy_value.max(0)[1].view(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define policy and target functions.\n",
    "policy = DQN(9,105,60,9)\n",
    "target = DQN(9,105,60,9)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "target.eval()\n",
    "\n",
    "# Declare optimizer:\n",
    "optimizer = torch.optim.RMSprop(policy.parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize board:\n",
    "board = TicTacToe_Board()\n",
    "\n",
    "# Initialize AI player:\n",
    "player1 = AIAgent(policy, target=target, train=True)\n",
    "\n",
    "# Initialize hardcode player:\n",
    "player2 = hardcode()\n",
    "player3 = RandomAgent()\n",
    "\n",
    "# Initialize replaymemory.\n",
    "replaymemory = ReplayMemory(CAPACITY)\n",
    "\n",
    "loss_list, reward_list = [], []\n",
    "epoch = 0\n",
    "win, drawn, lost, cheat, total = 0, 0, 0, 0, 0\n",
    "\n",
    "# Settings for Tensorboard:\n",
    "writer = SummaryWriter(log_dir='tictactoe_train')\n",
    "# Start training loop:\n",
    "for episode in range(1, EPISODES):\n",
    "    total_reward = 0\n",
    "    board.reset()\n",
    "    state = board.get_state().copy()\n",
    "    \n",
    "    while not board.end:\n",
    "        # Player one makes move:\n",
    "        EPS = greedyPolicy(epoch, EPSmax, EPSmin, decay)\n",
    "        action = player1.select_action_train(state, EPS=EPS)\n",
    "        index = board.action2index(action.item())\n",
    "        new_state, reward = board.play(index[0], index[1])\n",
    "        if board.end or reward == -10:\n",
    "            # The agent either won or tried to cheat. Save movement to replaymemory.\n",
    "            replaymemory.add(torch.Tensor(state),\\\n",
    "                             action,\\\n",
    "                             torch.Tensor([reward]),\\\n",
    "                             torch.Tensor(new_state),\\\n",
    "                             board.end)\n",
    "            board.end = True\n",
    "            \n",
    "        if not board.end:\n",
    "            # Second player makes its move:\n",
    "            \n",
    "            # Get available actions from board:\n",
    "            avail_act = board.avail_actions(new_state)\n",
    "            # With certain probability execute random action.\n",
    "            if random.random() < 5e-2:\n",
    "                rnd_act = player3.select_action(new_state, avail_act)\n",
    "            else:\n",
    "                # Hardcode action\n",
    "                rnd_act = player2.select_action(new_state, avail_act)\n",
    "            index = board.action2index(rnd_act)\n",
    "            new_state, reward = board.play(index[0], index[1])\n",
    "            \n",
    "            # Save point to replaymemory:\n",
    "            replaymemory.add(torch.Tensor(state),\\\n",
    "                             action,\\\n",
    "                             torch.Tensor([reward]),\\\n",
    "                             torch.Tensor(new_state),\\\n",
    "                             board.end)\n",
    "        \n",
    "        state[:] = new_state\n",
    "        \n",
    "        # Optimization step:\n",
    "        loss = optimize(player1.policy, player1.target, replaymemory, BATCH,optimizer)\n",
    "            \n",
    "        epoch += 1\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "    if episode % UPDATE == 0:\n",
    "        player1.target.load_state_dict(player1.policy.state_dict())\n",
    "        \n",
    "    # Statistics after episode.\n",
    "    total += 1\n",
    "    if reward == -10:\n",
    "        cheat += 1\n",
    "    elif reward == 100:\n",
    "        win += 1\n",
    "    elif reward == -1:\n",
    "        lost += 1\n",
    "    elif reward == 0:\n",
    "        drawn += 1\n",
    "        \n",
    "    if episode % IT_PRINT == 0:\n",
    "        writer.add_scalars('Statistics', {'Win':torch.Tensor([win / total]),\\\n",
    "                                          'Loss': torch.Tensor([lost / total]),\\\n",
    "                                          'Drawn': torch.Tensor([drawn / total]),\\\n",
    "                                          'Cheat': torch.Tensor([cheat / total])}, episode)\n",
    "        writer.add_scalar('Loss', torch.Tensor([loss]), episode)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save model\n",
    "torch.save(player1.policy.state_dict(), 'AIagent.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
